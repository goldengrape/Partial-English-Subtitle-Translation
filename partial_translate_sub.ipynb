{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英语字幕加生词中文注释\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要使用ECDICT https://github.com/skywind3000/ECDICT/ 请从该网站下载ecdict.csv和stardict.py文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysubs2\n",
    "import re\n",
    "from stardict import DictCsv\n",
    "# from stardict import LemmaDB\n",
    "# from googletrans import Translator\n",
    "# import pandas\n",
    "from difflib import SequenceMatcher \n",
    "import itertools\n",
    "import string\n",
    "import argparse\n",
    "\n",
    "\n",
    "# depend on https://github.com/skywind3000/ECDICT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了查到单词在句子中的意思, 所以使用了机器翻译, 现在采用的是彩云小译的翻译API, \n",
    "请先至[彩云科技开放平台](https://dashboard.caiyunapp.com/user/sign_in/)注册账号，申请开通小译 Token。并将token存储在token.txt文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranlate(source, token):\n",
    "\n",
    "    import requests\n",
    "    import json\n",
    "    \n",
    "    url = \"http://api.interpreter.caiyunai.com/v1/translator\"\n",
    "    \n",
    "    #WARNING, this token is a test token for new developers, and it should be replaced by your token\n",
    "#     token = \"3975l6lr5pcbvidl6jl2\"\n",
    "    \n",
    "    \n",
    "    payload = {\n",
    "            \"source\" : source, \n",
    "            \"trans_type\" : \"en2zh\",\n",
    "            \"request_id\" : \"demo\",\n",
    "            \"detect\": True,\n",
    "            }\n",
    "    \n",
    "    headers = {\n",
    "            'content-type': \"application/json\",\n",
    "            'x-authorization': \"token \" + token,\n",
    "    }\n",
    "    \n",
    "    response = requests.request(\"POST\", url, data=json.dumps(payload), headers=headers)\n",
    "\n",
    "    return json.loads(response.text)['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longestSubstring(str1,str2): \n",
    "     # 两个字符串最长公共字符串\n",
    "     # initialize SequenceMatcher object with  \n",
    "     # input string \n",
    "    seqMatch = SequenceMatcher(None,str1,str2) \n",
    "  \n",
    "     # find match of longest sub-string \n",
    "     # output will be like Match(a=0, b=0, size=5) \n",
    "    match = seqMatch.find_longest_match(0, len(str1), 0, len(str2)) \n",
    "  \n",
    "     # print longest substring \n",
    "    if (match.size!=0): \n",
    "          return (str1[match.a: match.a + match.size])  \n",
    "    else: \n",
    "          return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所谓取得句子中的词义, 其实就是\n",
    "1. 取得句子的机器翻译\n",
    "2. 查到单词的本地翻译\n",
    "3. 找到两者的最长公共字符串\n",
    "4. 如果实在没有的话, 就让翻译API给个词义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trans(word_trans_from_dict, word_trans_from_translator, sentence_trans):\n",
    "    # 句子中的单词含义, 如果没有公共的, 就返回查到的词\n",
    "    match=longestSubstring(sentence_trans,word_trans_from_dict).replace('\\n',\"\").replace(\" \",\"\")\n",
    "    if match==\"\":\n",
    "        return word_trans_from_translator.replace(\"\\n\", \"\")\n",
    "    else:\n",
    "        return match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对生词的判定, 有几个可供选择的指标:\n",
    "* 单词中的标记, 比如是否是cet4/cet6/toelf/gre/, 需要排除的使用False标记, 必须包含的使用True标记, 无所谓的不写.\n",
    "* collins星级, 越小越难\n",
    "* 英国国家语料库词频顺序bnc, 越大越难\n",
    "* 当代语料库词频顺序frq, 越大越难\n",
    "\n",
    "目前是4个条件, 满足3个或者以上就纳入为生词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_unknown(word_query):\n",
    "    if not(word_query):\n",
    "        return False\n",
    "    # 是否认识?\n",
    "    tag_check={'zk':False,'cet4':False, 'cet6':False, 'toelf':True ,\"gre\":True,'ielts':True}\n",
    "    collins_threshold=2; collins_default=True\n",
    "    bnc_threshold=5000; bnc_default=True\n",
    "    frq_threshold=5000; frq_default=True\n",
    "    \n",
    "    # check tag\n",
    "    chk1=True\n",
    "    chk2=True\n",
    "    for (key, value) in tag_check.items():\n",
    "        if not(value):\n",
    "            chk1 = chk1 and not((key in word_query['tag']) if word_query['tag'] else True) \n",
    "        else: \n",
    "            chk2 = chk2 or ((key in word_query['tag']) if word_query['tag'] else True)\n",
    "    tag_chk = chk1 and chk2\n",
    "    \n",
    "    # check collins\n",
    "    collins_chk = (word_query['collins']<=collins_threshold) if word_query['collins']>=0 else collins_default\n",
    "    \n",
    "    # check bnc\n",
    "    bnc_chk=(word_query['bnc']>=bnc_threshold) if word_query['bnc']>0 else bnc_default\n",
    "    \n",
    "    # check frq\n",
    "    frq_chk=(word_query['frq']>=frq_threshold) if word_query['bnc']>0 else frq_default\n",
    "\n",
    "    return (tag_chk+collins_chk+bnc_chk+frq_chk) >=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是就一句一句地处理呗.\n",
    "* 先在本地查询单词, 查不到的话, 就随便先按上一个‘unkown’的词义\n",
    "* 然后看这个单词是否是生词, 如果是的话, 就把它加入到words_to_trans的字典中\n",
    "* 如果words_to_trans的字典是空的, 那这句话就不用做啥了, 直接返回\n",
    "* 如果words_to_trans的字典非空, 就把整句和要查的生词包在一起, 发给彩云小译翻译\n",
    "* 在按照前面说的求最长公共字符串, 找到单词的含义\n",
    "* 把每个带有注释的单词重新插入到句子中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trans_to_sentence(s, sdict, token, filter_word=True):\n",
    "    sentence=s.replace(\"\\\\N\", \" \").replace(\"\\n\", \" \")\n",
    "    words=sentence.split()\n",
    "    words_to_trans={}\n",
    "    for word in words:\n",
    "        word_query=sdict.query(word) if sdict.query(word) else sdict.query('unknown')   \n",
    "        if filter_word:\n",
    "            if word_unknown(word_query):\n",
    "                words_to_trans[word]=word_query['translation']\n",
    "        else:\n",
    "            words_to_trans[word]=word_query['translation']\n",
    "    if words_to_trans: # if words_to_trans is not empty\n",
    "        to_trans_list=[[sentence], words_to_trans.keys()]\n",
    "        to_trans_list=list(itertools.chain(*to_trans_list))\n",
    "        trans=tranlate(to_trans_list,token)\n",
    "        sentence_trans=trans[0]\n",
    "        word_with_trans={}\n",
    "        for idx, word in enumerate(words_to_trans.keys()):\n",
    "            word_trans=trans[idx+1]\n",
    "            word_with_trans[word]=get_trans(words_to_trans[word], word_trans, sentence_trans)\n",
    "        for (word, meaning) in word_with_trans.items():\n",
    "            meaning=word+\"(\"+meaning+\")\"\n",
    "            s=s[0:s.find(word)]+meaning+s[(s.find(word)+len(word)):]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合在一起, 处理整个字幕文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sub(sub_filename, output_filename, dict_filename, token_filename):\n",
    "    subs = pysubs2.load(sub_filename, encoding=\"utf-8\")\n",
    "    with open(token_filename, 'r') as f:\n",
    "        token=f.read()\n",
    "    sdict=DictCsv(dict_filename)\n",
    "    \n",
    "    for idx, line in enumerate(subs):\n",
    "        s=line.text\n",
    "        line.text=add_trans_to_sentence(s, sdict, token)\n",
    "    subs.save(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从命令行输入参数处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_filename=\"ecdict.csv\"\n",
    "# token_filename='token.txt'\n",
    "# input_filename=\"The.Witcher.S01E01.WEBRip.x264-ION10.srt\"\n",
    "# output_filename=\"my_subtitles_edited.srt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser(description='Process subtitle.')\n",
    "    parser.add_argument('-i', '--input', dest=\"input_filename\")\n",
    "    parser.add_argument('-o', '--output', dest=\"output_filename\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    process_sub(args.input_filename, \n",
    "                args.output_filename, \n",
    "                dict_filename=\"ecdict.csv\", \n",
    "                token_filename='token.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
