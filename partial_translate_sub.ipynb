{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英语字幕加生词中文注释\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要使用ECDICT https://github.com/skywind3000/ECDICT/ 请从该网站下载ecdict.csv和stardict.py文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysubs2\n",
    "import re\n",
    "from stardict import DictCsv\n",
    "# from stardict import LemmaDB\n",
    "# from googletrans import Translator\n",
    "# import pandas\n",
    "from difflib import SequenceMatcher \n",
    "import itertools\n",
    "import string\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# depend on https://github.com/skywind3000/ECDICT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了查到单词在句子中的意思, 所以使用了机器翻译, 现在采用的是彩云小译的翻译API, \n",
    "请先至[彩云科技开放平台](https://dashboard.caiyunapp.com/user/sign_in/)注册账号，申请开通小译 Token。并将token存储在token.txt文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranlate(source, token):\n",
    "\n",
    "    import requests\n",
    "    import json\n",
    "    \n",
    "    url = \"http://api.interpreter.caiyunai.com/v1/translator\"\n",
    "    \n",
    "    #WARNING, this token is a test token for new developers, and it should be replaced by your token\n",
    "#     token = \"3975l6lr5pcbvidl6jl2\"\n",
    "    \n",
    "    \n",
    "    payload = {\n",
    "            \"source\" : source, \n",
    "            \"trans_type\" : \"en2zh\",\n",
    "            \"request_id\" : \"demo\",\n",
    "            \"detect\": True,\n",
    "            }\n",
    "    \n",
    "    headers = {\n",
    "            'content-type': \"application/json\",\n",
    "            'x-authorization': \"token \" + token,\n",
    "    }\n",
    "    \n",
    "    response = requests.request(\"POST\", url, data=json.dumps(payload), headers=headers)\n",
    "\n",
    "    return json.loads(response.text)['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longestSubstring(str1,str2): \n",
    "     # 两个字符串最长公共字符串\n",
    "     # initialize SequenceMatcher object with  \n",
    "     # input string \n",
    "    seqMatch = SequenceMatcher(None,str1,str2) \n",
    "  \n",
    "     # find match of longest sub-string \n",
    "     # output will be like Match(a=0, b=0, size=5) \n",
    "    match = seqMatch.find_longest_match(0, len(str1), 0, len(str2)) \n",
    "  \n",
    "     # print longest substring \n",
    "    if (match.size!=0): \n",
    "          return (str1[match.a: match.a + match.size])  \n",
    "    else: \n",
    "          return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所谓取得句子中的词义, 其实就是\n",
    "1. 取得句子的机器翻译\n",
    "2. 查到单词的本地翻译\n",
    "3. 找到两者的最长公共字符串\n",
    "4. 如果实在没有的话, 就让翻译API给个词义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trans(word_trans_from_dict, word_trans_from_translator, sentence_trans):\n",
    "    # 句子中的单词含义, 如果没有公共的, 就返回查到的词\n",
    "    match=longestSubstring(sentence_trans,word_trans_from_dict)\n",
    "    match=re.sub('[a-zA-Z0-9.\\n ]*',\"\",match) #只留下中文\n",
    "    exclude_list=[\"要\",\"着\",\"了\",\"过\",\"来\",\"的\",\"是\",\"说\",\"去\",\"到\",\"给\",\"做\",\"有\",\"看\",\"操\"]\n",
    "    if any(match == e for e in exclude_list):\n",
    "        match=\"\"\n",
    "    if match==\"\":\n",
    "        return re.sub('[a-zA-Z0-9.\\n ]*',\"\",word_trans_from_translator)\n",
    "    else:\n",
    "        return match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对生词的判定, 有几个可供选择的指标:\n",
    "* 单词中的标记, 比如是否是cet4/cet6/toelf/gre/, 需要排除的使用False标记, 必须包含的使用True标记, 无所谓的不写.\n",
    "* collins星级, 越小越难\n",
    "* 英国国家语料库词频顺序bnc, 越大越难\n",
    "* 当代语料库词频顺序frq, 越大越难\n",
    "\n",
    "目前是4个条件, 满足3个或者以上就纳入为生词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_unknown(word_query, word_judge,exclude_word_list):\n",
    "    if not(word_query):\n",
    "        return False #查不到就算了\n",
    "    # check in exclude_word_list\n",
    "    if word_query['word'] in exclude_word_list:\n",
    "        return False\n",
    "    \n",
    "    # 是否认识?\n",
    "    include_tag=word_judge[\"include_tag\"] \n",
    "    exclude_tag=word_judge[\"exclude_tag\"]\n",
    "\n",
    "\n",
    "    collins_threshold=word_judge[\"collins_threshold\"]; collins_default=True\n",
    "    bnc_threshold=word_judge[\"bnc_threshold\"]; bnc_default=True\n",
    "    frq_threshold=word_judge[\"frq_threshold\"]; frq_default=True\n",
    "    \n",
    "    # check tag\n",
    "    include_list=include_tag.lower().split()\n",
    "    exclude_list=exclude_tag.lower().split()\n",
    "    if word_query['tag']: # 如果该单词有tag标记\n",
    "        word_tag=word_query['tag'] \n",
    "        tag_chk=(not(any(e in word_tag for e in exclude_list)) \n",
    "                 and \n",
    "                 any(i in word_tag for i in include_list))    \n",
    "    else:\n",
    "        tag_chk=True  #如果该单词没有tag标记, 默认为\n",
    "    \n",
    "    # check collins\n",
    "    collins_chk = (word_query['collins']<=collins_threshold) if word_query['collins']>=0 else collins_default\n",
    "    \n",
    "    # check bnc\n",
    "    bnc_chk=(word_query['bnc']>=bnc_threshold) if word_query['bnc']>0 else bnc_default\n",
    "    \n",
    "    # check frq\n",
    "    frq_chk=(word_query['frq']>=frq_threshold) if word_query['bnc']>0 else frq_default\n",
    "    \n",
    "    \n",
    "\n",
    "    return (tag_chk+collins_chk+bnc_chk+frq_chk) >=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是就一句一句地处理呗.\n",
    "* 先在本地查询单词, 查不到的话, 就随便先按上一个‘unkown’的词义\n",
    "* 然后看这个单词是否是生词, 如果是的话, 就把它加入到words_to_trans的字典中\n",
    "* 如果words_to_trans的字典是空的, 那这句话就不用做啥了, 直接返回\n",
    "* 如果words_to_trans的字典非空, 就把整句和要查的生词包在一起, 发给彩云小译翻译\n",
    "* 在按照前面说的求最长公共字符串, 找到单词的含义\n",
    "* 把每个带有注释的单词重新插入到句子中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trans_to_sentence(s, sdict, token, word_judge, exclude_word_list, filter_word=True):\n",
    "    sentence=s.replace(\"\\\\N\", \" \").replace(\"\\n\", \" \")\n",
    "    words=sentence.split()\n",
    "    words_to_trans={}\n",
    "    for word in words:\n",
    "        word_query=sdict.query(word) if sdict.query(word) else sdict.query('unknown')   \n",
    "        if filter_word:\n",
    "            if word_unknown(word_query,word_judge,exclude_word_list):\n",
    "                words_to_trans[word]=word_query['translation']\n",
    "        else:\n",
    "            words_to_trans[word]=word_query['translation']\n",
    "    if words_to_trans: # if words_to_trans is not empty\n",
    "        to_trans_list=[[sentence], words_to_trans.keys()]\n",
    "        to_trans_list=list(itertools.chain(*to_trans_list))\n",
    "        trans=tranlate(to_trans_list,token)\n",
    "        sentence_trans=trans[0]\n",
    "        word_with_trans={}\n",
    "        for idx, word in enumerate(words_to_trans.keys()):\n",
    "            word_trans=trans[idx+1]\n",
    "            word_with_trans[word]=get_trans(words_to_trans[word], word_trans, sentence_trans)\n",
    "        for (word, meaning) in word_with_trans.items():\n",
    "            meaning=word+\"(\"+meaning+\")\"\n",
    "            s=s[0:s.find(word)]+meaning+s[(s.find(word)+len(word)):]\n",
    "    return s, list(words_to_trans.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合在一起, 处理整个字幕文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sub(sub_filename, output_filename, word_judge,\n",
    "                dict_filename, token_filename\n",
    "               ):\n",
    "    subs = pysubs2.load(sub_filename, encoding=\"utf-8\")\n",
    "    with open(token_filename, 'r') as f:\n",
    "        token=f.read()\n",
    "    sdict=DictCsv(dict_filename)\n",
    "    \n",
    "    try:\n",
    "        with open(word_judge['exclude_word_filename'], 'r') as f:\n",
    "            exclude_word_list=f.read()\n",
    "    except:\n",
    "        exclude_word_list=\"\"\n",
    "    \n",
    "    words_translated=[]\n",
    "    for idx, line in enumerate(subs):\n",
    "        s=line.text\n",
    "        line.text, w_t=add_trans_to_sentence(s, sdict, token, word_judge, exclude_word_list)\n",
    "        words_translated.append(w_t)\n",
    "    subs.save(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从命令行输入参数处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_filename=\"ecdict.csv\"\n",
    "# token_filename='token.txt'\n",
    "# input_filename=\"The.Witcher.S01E01.WEBRip.x264-ION10.srt\"\n",
    "# output_filename=\"my_subtitles_edited.srt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser(description='Process subtitle.')\n",
    "    parser.add_argument('-i', '--input', dest=\"input_filename\", help=\"需要处理的英文字幕文件\")\n",
    "    parser.add_argument('-o', '--output', dest=\"output_filename\", help='输出文件')\n",
    "    parser.add_argument('-include', nargs='?', dest=\"include_tag\", type=str,\n",
    "                        help='生词的定义: 包含哪些标记, 用空格隔开, 例如 cet6 toelf gre ielts',\n",
    "                       default=\"cet6 gre ielts\")\n",
    "    parser.add_argument('-exclude', nargs='?', dest='exclude_tag', type=str,\n",
    "                        help='生词的定义: 除外哪些标记, 用空格隔开, 例如 zk gk cet4',\n",
    "                       default=\"zk gk cet4\")\n",
    "    parser.add_argument('-collins', nargs='?',dest='collins_threshold', type=int, \n",
    "                        help='collins星级', default=2)\n",
    "    parser.add_argument('-bnc', nargs='?', dest='bnc_threshold', type=int,\n",
    "                       help='英国国家语料库词频顺序bnc, 越大越难', default=5000)\n",
    "    parser.add_argument('-frq', nargs='?', dest='frq_threshold', type=int,\n",
    "                       help='当代语料库词频顺序frq, 越大越难', default=5000)\n",
    "    parser.add_argument('-e', '--exclude_word', nargs='?', dest='exclude_word_filename', \n",
    "                        type=str, help='需要排除的单词列表, txt文件, 每行一个单词', \n",
    "                        default=\"exclude_word_list.txt\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    word_judge={}\n",
    "    word_judge[\"include_tag\"]=args.include_tag \n",
    "    word_judge[\"exclude_tag\"]=args.exclude_tag \n",
    "    word_judge[\"collins_threshold\"]=args.collins_threshold \n",
    "    word_judge[\"bnc_threshold\"]=args.bnc_threshold \n",
    "    word_judge['frq_threshold']=args.frq_threshold \n",
    "    word_judge['exclude_word_filename']=args.exclude_word_filename\n",
    "    \n",
    "    \n",
    "    process_sub(args.input_filename, \n",
    "                args.output_filename, \n",
    "                word_judge,\n",
    "                dict_filename=\"ecdict.csv\", \n",
    "                token_filename='token.txt'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_filename=\"ecdict.csv\"\n",
    "# sdict=DictCsv(dict_filename)\n",
    "# with open(\"exclude_word_list.txt\", 'r') as f:\n",
    "#     e=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
